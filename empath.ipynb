{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: empath in d:\\envs\\annotate\\lib\\site-packages (0.89)\n",
      "Requirement already satisfied: requests in d:\\envs\\annotate\\lib\\site-packages (from empath) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\envs\\annotate\\lib\\site-packages (from requests->empath) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\envs\\annotate\\lib\\site-packages (from requests->empath) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\envs\\annotate\\lib\\site-packages (from requests->empath) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\envs\\annotate\\lib\\site-packages (from requests->empath) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score means number of words in the input text that match the words associated with that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'violence': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.analyze(\"he hit the other person\", categories=[\"violence\"])\n",
    "# => {'violence': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize: divide by the total number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'violence': 0.2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.analyze(\"he hit the other person\", categories=[\"violence\"], normalize=True)\n",
    "# => {'violence': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fail to connect with external server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexicon.create_category(\"colors\",[\"red\",\"blue\",\"green\"])\n",
    "# => [\"blue\", \"green\", \"purple\", \"purple\", \"green\", \"yellow\", \"red\", \"grey\", \"violet\", \"gray\", \"blue\", \"orange\", \"white\", \"pink\", \"yellow\", \"black\", \"brown\", \"brown\", \"red\", \"aqua\", \"turquoise\", \"blue_color\", \"colored\", \"color\", \"same_shade\", \"violet\", \"gray\", \"grey\", \"teal\", \"nice_shade\", \"coloured\", \"forest_green\", \"colored\", \"different_shade\", \"colour\", \"sparkly\", \"reddish\", \"beautiful_shade\", \"greenish\", \"indigo\", \"darker_shade\", \"emerald\", \"lovely_shade\", \"tints\", \"crimson\", \"dark_purple\", \"pink\", \"emerald\", \"sapphire\", \"golden\", \"lighter_shade\", \"lime_green\", \"coloured\", \"bright\", \"same_color\", \"specks\", \"red\", \"golden_color\", \"different_shades\", \"chocolate_brown\", \"orange\", \"bluish\", \"green\", \"deep_purple\", \"magenta\", \"green_color\", \"dark_shade\", \"bright_orange\", \"milky\", \"lilac\", \"light_brown\", \"sparkling\", \"golden_brown\", \"silvery\", \"baby_blue\", \"blood_red\", \"pink\", \"teal\", \"blue\", \"yellowish\", \"turquoise\", \"same_colour\", \"sparkly\", \"aquamarine\", \"black_color\", \"white\", \"cerulean\", \"perfect_shade\", \"dark\", \"speckled\", \"charcoal\", \"greyish\", \"midnight_blue\", \"emerald_green\", \"deep_brown\", \"ocean_blue\", \"flecks\", \"amber\", \"pinkish\", \"jet_black\"]\n",
    "\n",
    "#lexicon.analyze(\"My favorite color is blue\", categories=[\"colors\"], normalize=True)\n",
    "# => {'colors': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 1: one paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\envs\\annotate\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\envs\\annotate\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\envs\\annotate\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.6 MB 9.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/11.6 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.1/11.6 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.6 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 9.1 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Category     Score\n",
      "8                cold  0.031579\n",
      "103           weather  0.031579\n",
      "136            warmth  0.031579\n",
      "3               money  0.021053\n",
      "25         government  0.021053\n",
      "58       irritability  0.010526\n",
      "101          sympathy  0.010526\n",
      "105             trust  0.010526\n",
      "114        politeness  0.010526\n",
      "35             school  0.010526\n",
      "127              gain  0.010526\n",
      "132           science  0.010526\n",
      "151          politics  0.010526\n",
      "156        technology  0.010526\n",
      "158    shape_and_size  0.010526\n",
      "163         terrorism  0.010526\n",
      "175         messaging  0.010526\n",
      "192  positive_emotion  0.010526\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Perform sentiment analysis\n",
    "result = lexicon.analyze(\n",
    "    \"Despite decades of scientific consensus on the importance of urgent climate action, a significant subset of global citizens continues to be doubtful, even dismissive of climate change science. Among these, there has lately been a growing contingent â€“ individuals who have not necessarily declared allegiance to the contrarian side en masse, but instead display scattered, piecemeal scepticism on this increasingly pressing issue. Weary to unravel the complexities surrounding their doubt-some views, The NZ Herald initiated an in-depth inquiry; endeavouring to delve deeper and dispel the haze enveloping understanding of climate change amongst New Zealand communities.\", \n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "df = pd.DataFrame(list(result.items()), columns=['Category', 'Score'])\n",
    "\n",
    "# Filter for scores greater than 0\n",
    "filtered_df = df[df['Score'] > 0.0]\n",
    "\n",
    "# Sort the DataFrame by the Score column in descending order\n",
    "sorted_df = filtered_df.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(sorted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2.1: dataset of twitter posts on climate change from Kaggle -  tweet by tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Category     Score                                              Tweet\n",
      "0   government  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "1    superhero  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "2      healing  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "3       heroic  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "4        trust  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "5    terrorism  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "6      monster  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "7       giving  0.090909  RT @csmonitor: Could mutant plants save us fro...\n",
      "8        money  0.041667  RT @SethMacFarlane: HRC proposes installing ha...\n",
      "9        sleep  0.041667  RT @SethMacFarlane: HRC proposes installing ha...\n",
      "10        cold  0.041667  RT @SethMacFarlane: HRC proposes installing ha...\n",
      "11     weather  0.041667  RT @SethMacFarlane: HRC proposes installing ha...\n",
      "12      warmth  0.041667  RT @SethMacFarlane: HRC proposes installing ha...\n",
      "13  technology  0.041667  RT @SethMacFarlane: HRC proposes installing ha...\n",
      "14        cold  0.117647  Arctic ice melt could trigger uncontrollable c...\n",
      "15       money  0.058824  Arctic ice melt could trigger uncontrollable c...\n",
      "16  government  0.058824  Arctic ice melt could trigger uncontrollable c...\n",
      "17     weather  0.058824  Arctic ice melt could trigger uncontrollable c...\n",
      "18        fire  0.058824  Arctic ice melt could trigger uncontrollable c...\n",
      "19        rage  0.058824  Arctic ice melt could trigger uncontrollable c...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset of Twitter posts\n",
    "df_tweets = pd.read_csv('twitter_sentiment_data.csv')\n",
    "\n",
    "# Sample 100 tweets from the dataset\n",
    "df_sample = df_tweets.sample(100)\n",
    "\n",
    "# Create an empty list to hold the raw analysis results\n",
    "analysis_results = []\n",
    "\n",
    "# Loop through each tweet and analyze it using Empath\n",
    "for tweet in df_sample['message']:  \n",
    "    result = lexicon.analyze(tweet, normalize=True)\n",
    "    \n",
    "    # Append the result (raw analysis) to the results list\n",
    "    analysis_results.append(result)\n",
    "\n",
    "# Now convert the raw results to a DataFrame and filter out categories with score 0\n",
    "all_data = []  # List to hold the final data for DataFrame\n",
    "\n",
    "for tweet, result in zip(df_sample['message'], analysis_results):\n",
    "    # Convert the result into a DataFrame\n",
    "    temp_df = pd.DataFrame(list(result.items()), columns=['Category', 'Score'])\n",
    "    \n",
    "    # Filter out categories with score 0\n",
    "    filtered_df = temp_df[temp_df['Score'] > 0.0]\n",
    "    \n",
    "    # Sort the filtered DataFrame by Score in descending order\n",
    "    sorted_df = filtered_df.sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    # Add a new column 'Tweet' to the sorted DataFrame for the corresponding tweet text\n",
    "    sorted_df['Tweet'] = tweet\n",
    "    \n",
    "    # Append the sorted DataFrame to the all_data list\n",
    "    all_data.append(sorted_df)\n",
    "\n",
    "# Concatenate all individual DataFrames into one\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "#final_df.to_csv('climate_change_analysis_results.csv', index=False)\n",
    "\n",
    "# Display the final result\n",
    "print(final_df.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2.2: dataset of twitter posts on climate change from Kaggle - aggregated scores of all the post text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Category  Total_Score\n",
      "7             warmth         93.0\n",
      "1               cold         78.0\n",
      "5            weather         76.0\n",
      "0              money         59.0\n",
      "2         government         36.0\n",
      "24         terrorism         29.0\n",
      "9   negative_emotion         15.0\n",
      "60     communication         13.0\n",
      "62          speaking         11.0\n",
      "19               war         11.0\n",
      "39          business         10.0\n",
      "18             fight          9.0\n",
      "81           meeting          8.0\n",
      "41            school          8.0\n",
      "63            listen          8.0\n",
      "83         messaging          8.0\n",
      "35            injury          8.0\n",
      "73           science          7.0\n",
      "40              work          7.0\n",
      "59           college          7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset of Twitter posts\n",
    "df_tweets = pd.read_csv('twitter_sentiment_data.csv')\n",
    "\n",
    "df_sample = df_tweets.sample(100)  # Sampling 100 tweets for the analysis\n",
    "\n",
    "# Create an empty dictionary to hold the total scores for each category\n",
    "total_scores = {}\n",
    "\n",
    "# Loop through each tweet and analyze it using Empath\n",
    "for tweet in df_sample['message']:  \n",
    "    result = lexicon.analyze(tweet, normalize=False)\n",
    "    \n",
    "    # Convert the result into a DataFrame and filter out categories with score 0\n",
    "    temp_df = pd.DataFrame(list(result.items()), columns=['Category', 'Score'])\n",
    "    filtered_df = temp_df[temp_df['Score'] > 0.0]\n",
    "    \n",
    "    # Add the scores to the total_scores dictionary\n",
    "    for category, score in filtered_df.values:\n",
    "        if category in total_scores:\n",
    "            total_scores[category] += score  # Accumulate the score for each category\n",
    "        else:\n",
    "            total_scores[category] = score  # Initialize the score for new category\n",
    "\n",
    "# Convert the total_scores dictionary into a DataFrame\n",
    "aggregated_df = pd.DataFrame(list(total_scores.items()), columns=['Category', 'Total_Score'])\n",
    "\n",
    "# Sort the DataFrame by Total_Score in descending order and get the top 20 categories\n",
    "top_20_df = aggregated_df.sort_values(by='Total_Score', ascending=False).head(20)\n",
    "\n",
    "# Display the top 20 topics\n",
    "print(top_20_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
