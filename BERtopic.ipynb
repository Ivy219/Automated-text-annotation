{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'd:\\envs\\annotate\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'd:\\envs\\annotate\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.40-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in d:\\envs\\annotate\\lib\\site-packages (from bertopic) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in d:\\envs\\annotate\\lib\\site-packages (from bertopic) (2.2.3)\n",
      "Collecting plotly>=4.7.0 (from bertopic)\n",
      "  Using cached plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in d:\\envs\\annotate\\lib\\site-packages (from bertopic) (1.5.2)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in d:\\envs\\annotate\\lib\\site-packages (from bertopic) (4.67.1)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in d:\\envs\\annotate\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.0 in d:\\envs\\annotate\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\envs\\annotate\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\envs\\annotate\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\envs\\annotate\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
      "Collecting tenacity>=6.2.0 (from plotly>=4.7.0->bertopic)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: packaging in d:\\envs\\annotate\\lib\\site-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\envs\\annotate\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\envs\\annotate\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.47.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\envs\\annotate\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.26.5)\n",
      "Requirement already satisfied: Pillow in d:\\envs\\annotate\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (11.0.0)\n",
      "Requirement already satisfied: colorama in d:\\envs\\annotate\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading numba-0.60.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in d:\\envs\\annotate\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\envs\\annotate\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\envs\\annotate\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\envs\\annotate\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\envs\\annotate\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.2->umap-learn>=0.5.0->bertopic)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\envs\\annotate\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\envs\\annotate\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\envs\\annotate\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\envs\\annotate\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\envs\\annotate\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\envs\\annotate\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\envs\\annotate\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\envs\\annotate\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\envs\\annotate\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\envs\\annotate\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
      "Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
      "Downloading hdbscan-0.8.40-cp310-cp310-win_amd64.whl (730 kB)\n",
      "   ---------------------------------------- 0.0/730.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 730.9/730.9 kB 10.0 MB/s eta 0:00:00\n",
      "Using cached plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 22.2 MB/s eta 0:00:00\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 5.0/203.1 MB 23.2 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 10.0/203.1 MB 23.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 15.2/203.1 MB 23.3 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 18.6/203.1 MB 21.7 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 22.8/203.1 MB 21.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 27.3/203.1 MB 21.1 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 32.5/203.1 MB 21.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 37.5/203.1 MB 21.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 42.7/203.1 MB 22.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 48.0/203.1 MB 22.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 53.2/203.1 MB 22.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 58.5/203.1 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 63.7/203.1 MB 22.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 68.4/203.1 MB 22.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 72.9/203.1 MB 22.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 77.9/203.1 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 82.8/203.1 MB 22.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 87.6/203.1 MB 22.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 92.5/203.1 MB 22.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 97.0/203.1 MB 22.7 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 101.7/203.1 MB 22.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 106.7/203.1 MB 22.7 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 111.1/203.1 MB 22.7 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 116.1/203.1 MB 22.7 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 120.8/203.1 MB 22.7 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 125.3/203.1 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 130.0/203.1 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 135.0/203.1 MB 22.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 139.7/203.1 MB 22.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 144.4/203.1 MB 22.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 149.2/203.1 MB 22.7 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 153.9/203.1 MB 22.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 158.3/203.1 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 163.1/203.1 MB 22.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 167.8/203.1 MB 22.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 172.5/203.1 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 177.2/203.1 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 182.2/203.1 MB 22.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 186.6/203.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 191.4/203.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 195.8/203.1 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.5/203.1 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 21.8 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 4.7/6.2 MB 23.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 21.0 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 4.7/28.1 MB 23.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 10.0/28.1 MB 23.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.9/28.1 MB 24.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 19.7/28.1 MB 23.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.6/28.1 MB 24.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.3 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, tenacity, sympy, networkx, llvmlite, torch, plotly, numba, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
      "Successfully installed bertopic-0.16.4 hdbscan-0.8.40 llvmlite-0.43.0 mpmath-1.3.0 networkx-3.4.2 numba-0.60.0 plotly-5.24.1 pynndescent-0.5.13 sentence-transformers-3.3.1 sympy-1.13.1 tenacity-9.0.0 torch-2.5.1 umap-learn-0.5.7\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial with one paragraph - data too small as BERTopic requires UMAP and HDBSCAN perform better with datasets of at least 50–100 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in d:\\envs\\annotate\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\envs\\annotate\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\envs\\annotate\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in d:\\envs\\annotate\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in d:\\envs\\annotate\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\envs\\annotate\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\envs\\annotate\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\envs\\annotate\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\envs\\annotate\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\envs\\annotate\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\envs\\annotate\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\envs\\annotate\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\envs\\annotate\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\envs\\annotate\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\envs\\annotate\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\envs\\annotate\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\envs\\annotate\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k must be less than or equal to the number of training points",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(umap_model\u001b[38;5;241m=\u001b[39mumap_model)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Fit the model to your dataset\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Reduce the number of topics\u001b[39;00m\n\u001b[0;32m     28\u001b[0m topic_model\u001b[38;5;241m.\u001b[39mreduce_topics(documents, nr_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32md:\\envs\\annotate\\lib\\site-packages\\bertopic\\_bertopic.py:463\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    459\u001b[0m         umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mtransform(embeddings)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(documents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;66;03m# Cluster reduced embeddings\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m     documents, probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cluster_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_zeroshot() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(assigned_documents) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    465\u001b[0m         documents, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_zeroshot_topics(\n\u001b[0;32m    466\u001b[0m             documents, embeddings, assigned_documents, assigned_embeddings\n\u001b[0;32m    467\u001b[0m         )\n",
      "File \u001b[1;32md:\\envs\\annotate\\lib\\site-packages\\bertopic\\_bertopic.py:3778\u001b[0m, in \u001b[0;36mBERTopic._cluster_embeddings\u001b[1;34m(self, umap_embeddings, documents, partial_fit, y)\u001b[0m\n\u001b[0;32m   3776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3778\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdbscan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3780\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhdbscan_model\u001b[38;5;241m.\u001b[39mfit(umap_embeddings)\n",
      "File \u001b[1;32md:\\envs\\annotate\\lib\\site-packages\\hdbscan\\hdbscan_.py:1270\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_ \u001b[38;5;241m=\u001b[39m new_probabilities\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_data:\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prediction_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_detection_data:\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_branch_detection_data()\n",
      "File \u001b[1;32md:\\envs\\annotate\\lib\\site-packages\\hdbscan\\hdbscan_.py:1311\u001b[0m, in \u001b[0;36mHDBSCAN.generate_prediction_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported for prediction data!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric))\n\u001b[0;32m   1309\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prediction_data \u001b[38;5;241m=\u001b[39m PredictionData(\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_data,\n\u001b[0;32m   1313\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondensed_tree_,\n\u001b[0;32m   1314\u001b[0m         min_samples,\n\u001b[0;32m   1315\u001b[0m         tree_type\u001b[38;5;241m=\u001b[39mtree_type,\n\u001b[0;32m   1316\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[0;32m   1317\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_kwargs\n\u001b[0;32m   1318\u001b[0m     )\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1320\u001b[0m     warn(\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot generate prediction data for non-vector\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace inputs -- access to the source data rather\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan mere distances is required!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m     )\n",
      "File \u001b[1;32md:\\envs\\annotate\\lib\\site-packages\\hdbscan\\prediction.py:103\u001b[0m, in \u001b[0;36mPredictionData.__init__\u001b[1;34m(self, data, condensed_tree, min_samples, tree_type, metric, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_type_map[tree_type](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data,\n\u001b[0;32m    102\u001b[0m                                            metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_samples\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_metric \u001b[38;5;241m=\u001b[39m DistanceMetric\u001b[38;5;241m.\u001b[39mget_metric(metric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m selected_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(condensed_tree\u001b[38;5;241m.\u001b[39m_select_clusters())\n",
      "File \u001b[1;32msklearn\\\\neighbors\\\\_binary_tree.pxi:1180\u001b[0m, in \u001b[0;36msklearn.neighbors._kd_tree.BinaryTree64.query\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k must be less than or equal to the number of training points"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Sample dataset (replace with your dataset)\n",
    "documents = [\n",
    "    \"Climate change is a pressing issue affecting the entire planet.\",\n",
    "    \"We need renewable energy solutions to combat global warming.\",\n",
    "    \"Carbon emissions have reached an all-time high in recent years.\",\n",
    "    \"Sustainability is key to preserving our environment.\",\n",
    "    \"What are the economic impacts of climate policies?\"\n",
    "]\n",
    "\n",
    "# Define a UMAP model with compatible parameters\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=2,  # Lower for small datasets\n",
    "    n_components=2,  # Reduced dimensions, small for small datasets\n",
    "    metric='cosine',  # Distance metric\n",
    "    init='random'  # Avoid spectral initialization issues\n",
    ")\n",
    "\n",
    "# Pass the custom UMAP model to BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Fit the model to your dataset\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "# Reduce the number of topics\n",
    "topic_model.reduce_topics(documents, nr_topics=5)\n",
    "\n",
    "# View the topics\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "# Get keywords for a specific topic\n",
    "print(topic_model.get_topic(0))  # Replace 0 with the topic ID of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial with transcript2008-2018.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Path to your ZIP file\n",
    "zip_path = r\"D:\\dev\\annotate\\Automated-text-annotation\\transcripts2008-2018.zip\"\n",
    "extracted_path = r\"D:\\dev\\annotate\\Automated-text-annotation\\Extract transcripts 2008-2018\"\n",
    "\n",
    "# Extract ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_path)\n",
    "\n",
    "# Load all `.txt` files into a list\n",
    "documents = []\n",
    "for file_name in os.listdir(extracted_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        file_path = os.path.join(extracted_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            documents.append(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic  Count                        Name  \\\n",
      "0      -1   1315            -1_the_and_to_of   \n",
      "1       0    100             0_and_the_of_to   \n",
      "2       1     77             1_the_to_in_and   \n",
      "3       2     74        2_universe_the_of_we   \n",
      "4       3     71          3_energy_the_we_to   \n",
      "..    ...    ...                         ...   \n",
      "57     56     12             56_la_da_li_heh   \n",
      "58     57     12       57_the_and_animals_to   \n",
      "59     58     11          58_the_and_in_that   \n",
      "60     59     11        59_fashion_to_the_it   \n",
      "61     60     10  60_blind_me_wheelchair_the   \n",
      "\n",
      "                                       Representation  \\\n",
      "0       [the, and, to, of, that, in, you, is, we, it]   \n",
      "1     [and, the, of, to, it, that, this, you, so, in]   \n",
      "2     [the, to, in, and, of, was, that, is, we, they]   \n",
      "3   [universe, the, of, we, is, that, this, and, i...   \n",
      "4   [energy, the, we, to, and, is, of, that, in, c...   \n",
      "..                                                ...   \n",
      "57  [la, da, li, heh, thank, sos, my, marries, jan...   \n",
      "58  [the, and, animals, to, in, of, lions, it, the...   \n",
      "59  [the, and, in, that, of, to, science, you, it,...   \n",
      "60  [fashion, to, the, it, that, clothes, and, of,...   \n",
      "61  [blind, me, wheelchair, the, to, my, and, in, ...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [Thank you so much.\\nI am a journalist.\\n\\nMy ...  \n",
      "1   [I was basically concerned about what was goin...  \n",
      "2   [There's a man by the name of Captain\\nWilliam...  \n",
      "3   [So in 1781, an English composer,\\ntechnologis...  \n",
      "4   [It was my first year\\nas an atmospheric scien...  \n",
      "..                                                ...  \n",
      "57  [♫ Feminists don't have a sense of humor. ♫\\n\\...  \n",
      "58  [Now, I'm an ethnobotanist.\\nThat's a scientis...  \n",
      "59  [What do you guys think?\\nFor those who watche...  \n",
      "60  [Let's talk about thrift.\\nThrift is a concept...  \n",
      "61  [So, stepping down out of the bus,\\nI headed b...  \n",
      "\n",
      "[62 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the model to your dataset\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "# View the topics\n",
    "print(topic_model.get_topic_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 18:27:57,014 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "# # Visualize the topics\n",
    "# topic_model.visualize_topics()\n",
    "\n",
    "# # Visualize topic probabilities\n",
    "# topic_model.visualize_barchart()\n",
    "\n",
    "# # Save the model for future use\n",
    "# topic_model.save(\"bertopic_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
