{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4d8266",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f841b8c-ca69-4082-a1b3-281f9f33e7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HUAWEI\\.conda\\envs\\Aspect-Based-Sentiment-Analysis\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 1.08k/1.08k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 438M/438M [00:19<00:00, 23.0MB/s] \n",
      "Some layers from the model checkpoint at absa/classifier-rest-0.2 were not used when initializing BertABSClassifier: ['dropout_379']\n",
      "- This IS expected if you are initializing BertABSClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertABSClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of BertABSClassifier were not initialized from the model checkpoint at absa/classifier-rest-0.2 and are newly initialized: ['dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 5.12MB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 86.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "import aspect_based_sentiment_analysis as absa\n",
    "\n",
    "nlp = absa.load()\n",
    "text = (\"We are great fans of Slack, but we wish the subscriptions \"\n",
    "        \"were more accessible to small startups.\")\n",
    "\n",
    "slack, price = nlp(text, aspects=['slack', 'price'])\n",
    "assert price.sentiment == absa.Sentiment.negative\n",
    "assert slack.sentiment == absa.Sentiment.positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61834b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: Price - Sentiment: 1\n",
      "Aspect: Slack - Sentiment: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Aspect: Price - Sentiment: {price.sentiment}\")\n",
    "print(f\"Aspect: Slack - Sentiment: {slack.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364affb1",
   "metadata": {},
   "source": [
    "## Trial on twitter_sentiment_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c1ac9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "     --------------------------------------- 10.0/10.0 MB 29.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d2ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblobNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting nltk>=3.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 19.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from nltk>=3.1->textblob) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from nltk>=3.1->textblob) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from tqdm->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: nltk, textblob\n",
      "Successfully installed nltk-3.8.1 textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cee08",
   "metadata": {},
   "source": [
    "### Read and pre-process data to remove stopwords and less informative tokens found by observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fdefc-2936-47e4-8d22-1f8005d8e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('twitter_sentiment_data.csv',nrows=1000)\n",
    "\n",
    "# Custom list of words to remove\n",
    "custom_stopwords = [\"climate\", \"change\", \"rt\", \"amp\"]\n",
    "\n",
    "\n",
    "# Preprocess tweets\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and token.lemma_ not in custom_stopwords]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['processed_text'] = df['message'].apply(preprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50c163",
   "metadata": {},
   "source": [
    "### Trial 1.1 Extract aspects with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9150911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['global', 'warming', 'trump', 'china', 'plan', 'exit', 'pact', 'rare', 'criticize', 'nt'], ['warming', 'global', 'documentary', 'leonardo', 'watch', 'dicaprio', 'cover', 'dicaprios', 'free', 'issue'], ['nt', 'believe', 'need', 'real', 'november', 'people', 'global', 'winter', 'shift', 'warming'], ['right', 'world', 'htt', 'watch', 'tackle', 'travel', 'mention', 'fight', 'cable', 'protect'], ['global', 'warming', 'hoax', 'help', 'trump', 'clinton', 'chinese', 'like', 'combat', 'email']]\n"
     ]
    }
   ],
   "source": [
    "# Extract aspects using LDA\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Get top words for each topic\n",
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(top_words)\n",
    "    return topics\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topics = get_top_words(lda, feature_names, 10)\n",
    "print (topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4808eb9",
   "metadata": {},
   "source": [
    "### use the sentiment score provided in the database\n",
    "\n",
    "- 2(News): the tweet links to factual news about climate change\n",
    "- 1(Pro): the tweet supports the belief of man-made climate change\n",
    "- 0(Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "- -1(Anti): the tweet does not believe in man-made climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a06b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: global, warming, trump, china, plan\n",
      "Average Sentiment: 0.08\n",
      "Tweet Count: 222\n",
      "\n",
      "Aspect: warming, global, documentary, leonardo, watch\n",
      "Average Sentiment: 0.08\n",
      "Tweet Count: 227\n",
      "\n",
      "Aspect: nt, believe, need, real, november\n",
      "Average Sentiment: 0.04\n",
      "Tweet Count: 183\n",
      "\n",
      "Aspect: right, world, htt, watch, tackle\n",
      "Average Sentiment: 0.20\n",
      "Tweet Count: 187\n",
      "\n",
      "Aspect: global, warming, hoax, help, trump\n",
      "Average Sentiment: 0.05\n",
      "Tweet Count: 181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df['sentiment_score'] = df['message'].apply(get_sentiment)\n",
    "\n",
    "# Assign topics to tweets\n",
    "doc_topics = lda.transform(doc_term_matrix)\n",
    "df['topic'] = doc_topics.argmax(axis=1)\n",
    "\n",
    "# Perform ABSA\n",
    "results = []\n",
    "for topic_idx, topic_words in enumerate(topics):\n",
    "    topic_tweets = df[df['topic'] == topic_idx]\n",
    "    avg_sentiment = topic_tweets['sentiment_score'].mean()\n",
    "    results.append({\n",
    "        'aspect': ', '.join(topic_words[:5]),\n",
    "        'sentiment': avg_sentiment,\n",
    "        'tweet_count': len(topic_tweets)\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Aspect: {result['aspect']}\")\n",
    "    print(f\"Average Sentiment: {result['sentiment']:.2f}\")\n",
    "    print(f\"Tweet Count: {result['tweet_count']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea5195",
   "metadata": {},
   "source": [
    "### Trial 1.2 Manually define aspects and use VADER sentiment library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab63006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "     -------------------------------------- 126.0/126.0 kB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from requests->vaderSentiment) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from requests->vaderSentiment) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huawei\\.conda\\envs\\aspect-based-sentiment-analysis\\lib\\site-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e542b51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect: Activism\n",
      "Average Sentiment: 0.07\n",
      "Tweet Count: 28\n",
      "\n",
      "Aspect: Economy\n",
      "Average Sentiment: -0.13\n",
      "Tweet Count: 19\n",
      "\n",
      "Aspect: Education\n",
      "Average Sentiment: 0.34\n",
      "Tweet Count: 3\n",
      "\n",
      "Aspect: Energy\n",
      "Average Sentiment: 0.26\n",
      "Tweet Count: 8\n",
      "\n",
      "Aspect: Environment\n",
      "Average Sentiment: -0.02\n",
      "Tweet Count: 5\n",
      "\n",
      "Aspect: Health\n",
      "Average Sentiment: 0.02\n",
      "Tweet Count: 18\n",
      "\n",
      "Aspect: Other\n",
      "Average Sentiment: 0.04\n",
      "Tweet Count: 738\n",
      "\n",
      "Aspect: Politics\n",
      "Average Sentiment: -0.12\n",
      "Tweet Count: 131\n",
      "\n",
      "Aspect: Science\n",
      "Average Sentiment: -0.02\n",
      "Tweet Count: 47\n",
      "\n",
      "Aspect: Technology\n",
      "Average Sentiment: 0.31\n",
      "Tweet Count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('twitter_sentiment_data.csv', nrows=1000)  # Using top 1000 rows as an example\n",
    "\n",
    "# Define aspects and associated keywords (as in the previous example)\n",
    "aspects = {\n",
    "    \"Politics\": [\"policy\", \"government\", \"legislation\", \"trump\", \"election\"],\n",
    "    \"Science\": [\"research\", \"study\", \"scientist\", \"evidence\", \"data\"],\n",
    "    \"Economy\": [\"business\", \"industry\", \"economy\", \"cost\", \"investment\"],\n",
    "    \"Environment\": [\"ecosystem\", \"biodiversity\", \"conservation\", \"pollution\"],\n",
    "    \"Energy\": [\"renewable\", \"fossil fuels\", \"solar\", \"wind\", \"carbon\"],\n",
    "    \"Activism\": [\"protest\", \"campaign\", \"awareness\", \"action\", \"movement\"],\n",
    "    \"Technology\": [\"innovation\", \"solution\", \"technology\", \"development\"],\n",
    "    \"Health\": [\"public health\", \"disease\", \"impact\", \"risk\", \"adaptation\"],\n",
    "    \"Education\": [\"awareness\", \"curriculum\", \"education\", \"inform\", \"learn\"]\n",
    "}\n",
    "\n",
    "def assign_aspect(tweet):\n",
    "    tweet_lower = tweet.lower()\n",
    "    for aspect, keywords in aspects.items():\n",
    "        if any(keyword in tweet_lower for keyword in keywords):\n",
    "            return aspect\n",
    "    return \"Other\"\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment scores\n",
    "def get_sentiment_scores(text):\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "# Apply aspect assignment and sentiment analysis\n",
    "df['aspect'] = df['message'].apply(assign_aspect)\n",
    "df['sentiment_scores'] = df['message'].apply(get_sentiment_scores)\n",
    "\n",
    "# Extract compound sentiment score\n",
    "df['sentiment'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Calculate average sentiment for each aspect\n",
    "aspect_results = df.groupby('aspect').agg({\n",
    "    'sentiment': 'mean',\n",
    "    'message': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Display results\n",
    "for _, row in aspect_results.iterrows():\n",
    "    print(f\"Aspect: {row['aspect']}\")\n",
    "    print(f\"Average Sentiment: {row['sentiment']:.2f}\")\n",
    "    print(f\"Tweet Count: {row['message']}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aspect-Based-Sentiment-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
